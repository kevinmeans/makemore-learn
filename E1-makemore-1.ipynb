{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f855093-d62c-446b-9516-18d551b51737",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "546e372b-6ae9-4cd3-b2b0-7b94b5f68b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f1434eb-a504-4910-9532-bb3ed0f1f510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "517135b9-5dfe-461f-9d23-439571970278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "24145e3c-586c-4e0a-9e5f-6822453691b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3815f947-abea-4e73-bcf4-31e8f5970d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bddce2ac-d936-4613-9cc2-aa45a192e3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([729, 27])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = torch.zeros((27*27+27, 27), dtype=torch.int32)\n",
    "N.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6d87c086-be71-4d14-a66f-ea59a143b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for word in words[:1]:\n",
    "    word = '.' + word + '.'\n",
    "    tuples = [([word[i],word[i+1]], word[i+2]) for i in range(len(word)-2)]\n",
    "    for [x1,x2],y in tuples:\n",
    "        ix1 = stoi[x1]\n",
    "        ix2 = stoi[x2]\n",
    "        iy = stoi[y]\n",
    "        xs.append((ix1,ix2))\n",
    "        ys.append(iy)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ef9e8f74-15ad-4c7c-b253-5bdb8b8a2c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5],\n",
       "        [ 5, 13],\n",
       "        [13, 13],\n",
       "        [13,  1]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "48fa1496-46bb-418d-9ab2-fdad193b345b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f50b2b49-6a20-424c-acc5-d52382793f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27*27).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fcde5b86-0a82-4cc4-961b-f6ac19dca047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 729])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9318119c-1ae1-4d71-852a-2f16b31723ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9db575a6-caa2-4c05-abce-af67958c9c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "04603896-8dd3-49aa-8392-d2efe9db580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*27, 27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1dc5c4f8-55fb-4a8d-b34b-584bf0bf9481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([729, 729])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b3daca25-b342-4335-a42f-1346b791e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27*27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "# btw: the last 2 lines here are together called a 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "53f45a55-8743-4769-a6e4-29b56f34931f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 27])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5a1b6e78-60dc-444b-95a7-3f47bd2d3a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7493, 0.1518, 0.4146, 0.1034, 0.1079, 0.3780, 0.2910, 0.4971,\n",
       "          0.6684, 0.5974, 0.1385, 0.6347, 0.5318, 0.3937, 0.8681, 0.9151,\n",
       "          0.7479, 0.4404, 0.2797, 0.2665, 0.4991, 0.3419, 0.0759, 0.9133,\n",
       "          0.5883, 0.9903, 0.8152],\n",
       "         [0.2507, 0.8482, 0.5854, 0.8966, 0.8921, 0.6220, 0.7090, 0.5029,\n",
       "          0.3316, 0.4026, 0.8615, 0.3653, 0.4682, 0.6063, 0.1319, 0.0849,\n",
       "          0.2521, 0.5596, 0.7203, 0.7335, 0.5009, 0.6581, 0.9241, 0.0867,\n",
       "          0.4117, 0.0097, 0.1848]],\n",
       "\n",
       "        [[0.5692, 0.6058, 0.4215, 0.6902, 0.8075, 0.6719, 0.3360, 0.6563,\n",
       "          0.1012, 0.7228, 0.9303, 0.2916, 0.5543, 0.4902, 0.5118, 0.7994,\n",
       "          0.4139, 0.3462, 0.6671, 0.8398, 0.7688, 0.8446, 0.8430, 0.1581,\n",
       "          0.1221, 0.0614, 0.8415],\n",
       "         [0.4308, 0.3942, 0.5785, 0.3098, 0.1925, 0.3281, 0.6640, 0.3437,\n",
       "          0.8988, 0.2772, 0.0697, 0.7084, 0.4457, 0.5098, 0.4882, 0.2006,\n",
       "          0.5861, 0.6538, 0.3329, 0.1602, 0.2312, 0.1554, 0.1570, 0.8419,\n",
       "          0.8779, 0.9386, 0.1585]],\n",
       "\n",
       "        [[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "          0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "          0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "          0.5000, 0.5000, 0.5000],\n",
       "         [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "          0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "          0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "          0.5000, 0.5000, 0.5000]],\n",
       "\n",
       "        [[0.7034, 0.9066, 0.5820, 0.7909, 0.5588, 0.4256, 0.2167, 0.6839,\n",
       "          0.9178, 0.7955, 0.0768, 0.9127, 0.1312, 0.7316, 0.4190, 0.3824,\n",
       "          0.6201, 0.6400, 0.4515, 0.1109, 0.3382, 0.2244, 0.7463, 0.4136,\n",
       "          0.6773, 0.5149, 0.6120],\n",
       "         [0.2966, 0.0934, 0.4180, 0.2091, 0.4412, 0.5744, 0.7833, 0.3161,\n",
       "          0.0822, 0.2045, 0.9232, 0.0873, 0.8688, 0.2684, 0.5810, 0.6176,\n",
       "          0.3799, 0.3600, 0.5485, 0.8891, 0.6618, 0.7756, 0.2537, 0.5864,\n",
       "          0.3227, 0.4851, 0.3880]]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b7a7a4d3-5649-41dc-907b-8f80fc1f6175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .m (indexes 0,13)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([[0.7493, 0.1518, 0.4146, 0.1034, 0.1079, 0.3780, 0.2910, 0.4971, 0.6684,\n",
      "         0.5974, 0.1385, 0.6347, 0.5318, 0.3937, 0.8681, 0.9151, 0.7479, 0.4404,\n",
      "         0.2797, 0.2665, 0.4991, 0.3419, 0.0759, 0.9133, 0.5883, 0.9903, 0.8152],\n",
      "        [0.2507, 0.8482, 0.5854, 0.8966, 0.8921, 0.6220, 0.7090, 0.5029, 0.3316,\n",
      "         0.4026, 0.8615, 0.3653, 0.4682, 0.6063, 0.1319, 0.0849, 0.2521, 0.5596,\n",
      "         0.7203, 0.7335, 0.5009, 0.6581, 0.9241, 0.0867, 0.4117, 0.0097, 0.1848]])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.3937307298183441\n",
      "log likelihood: -0.9320880174636841\n",
      "negative log likelihood: 0.9320880174636841\n",
      "--------\n",
      "bigram example 1: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([[0.7493, 0.1518, 0.4146, 0.1034, 0.1079, 0.3780, 0.2910, 0.4971, 0.6684,\n",
      "         0.5974, 0.1385, 0.6347, 0.5318, 0.3937, 0.8681, 0.9151, 0.7479, 0.4404,\n",
      "         0.2797, 0.2665, 0.4991, 0.3419, 0.0759, 0.9133, 0.5883, 0.9903, 0.8152],\n",
      "        [0.2507, 0.8482, 0.5854, 0.8966, 0.8921, 0.6220, 0.7090, 0.5029, 0.3316,\n",
      "         0.4026, 0.8615, 0.3653, 0.4682, 0.6063, 0.1319, 0.0849, 0.2521, 0.5596,\n",
      "         0.7203, 0.7335, 0.5009, 0.6581, 0.9241, 0.0867, 0.4117, 0.0097, 0.1848]])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.6062692403793335\n",
      "log likelihood: -0.5004311203956604\n",
      "negative log likelihood: 0.5004311203956604\n",
      "--------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([[0.5692, 0.6058, 0.4215, 0.6902, 0.8075, 0.6719, 0.3360, 0.6563, 0.1012,\n",
      "         0.7228, 0.9303, 0.2916, 0.5543, 0.4902, 0.5118, 0.7994, 0.4139, 0.3462,\n",
      "         0.6671, 0.8398, 0.7688, 0.8446, 0.8430, 0.1581, 0.1221, 0.0614, 0.8415],\n",
      "        [0.4308, 0.3942, 0.5785, 0.3098, 0.1925, 0.3281, 0.6640, 0.3437, 0.8988,\n",
      "         0.2772, 0.0697, 0.7084, 0.4457, 0.5098, 0.4882, 0.2006, 0.5861, 0.6538,\n",
      "         0.3329, 0.1602, 0.2312, 0.1554, 0.1570, 0.8419, 0.8779, 0.9386, 0.1585]])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.4902023375034332\n",
      "log likelihood: -0.71293705701828\n",
      "negative log likelihood: 0.71293705701828\n",
      "--------\n",
      "bigram example 2: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([[0.5692, 0.6058, 0.4215, 0.6902, 0.8075, 0.6719, 0.3360, 0.6563, 0.1012,\n",
      "         0.7228, 0.9303, 0.2916, 0.5543, 0.4902, 0.5118, 0.7994, 0.4139, 0.3462,\n",
      "         0.6671, 0.8398, 0.7688, 0.8446, 0.8430, 0.1581, 0.1221, 0.0614, 0.8415],\n",
      "        [0.4308, 0.3942, 0.5785, 0.3098, 0.1925, 0.3281, 0.6640, 0.3437, 0.8988,\n",
      "         0.2772, 0.0697, 0.7084, 0.4457, 0.5098, 0.4882, 0.2006, 0.5861, 0.6538,\n",
      "         0.3329, 0.1602, 0.2312, 0.1554, 0.1570, 0.8419, 0.8779, 0.9386, 0.1585]])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.5097976922988892\n",
      "log likelihood: -0.673741340637207\n",
      "negative log likelihood: 0.673741340637207\n",
      "--------\n",
      "bigram example 3: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.5\n",
      "log likelihood: -0.6931471824645996\n",
      "negative log likelihood: 0.6931471824645996\n",
      "--------\n",
      "bigram example 3: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.5\n",
      "log likelihood: -0.6931471824645996\n",
      "negative log likelihood: 0.6931471824645996\n",
      "--------\n",
      "bigram example 4: m. (indexes 13,0)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([[0.7034, 0.9066, 0.5820, 0.7909, 0.5588, 0.4256, 0.2167, 0.6839, 0.9178,\n",
      "         0.7955, 0.0768, 0.9127, 0.1312, 0.7316, 0.4190, 0.3824, 0.6201, 0.6400,\n",
      "         0.4515, 0.1109, 0.3382, 0.2244, 0.7463, 0.4136, 0.6773, 0.5149, 0.6120],\n",
      "        [0.2966, 0.0934, 0.4180, 0.2091, 0.4412, 0.5744, 0.7833, 0.3161, 0.0822,\n",
      "         0.2045, 0.9232, 0.0873, 0.8688, 0.2684, 0.5810, 0.6176, 0.3799, 0.3600,\n",
      "         0.5485, 0.8891, 0.6618, 0.7756, 0.2537, 0.5864, 0.3227, 0.4851, 0.3880]])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.7034233808517456\n",
      "log likelihood: -0.35179632902145386\n",
      "negative log likelihood: 0.35179632902145386\n",
      "--------\n",
      "bigram example 4: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([[0.7034, 0.9066, 0.5820, 0.7909, 0.5588, 0.4256, 0.2167, 0.6839, 0.9178,\n",
      "         0.7955, 0.0768, 0.9127, 0.1312, 0.7316, 0.4190, 0.3824, 0.6201, 0.6400,\n",
      "         0.4515, 0.1109, 0.3382, 0.2244, 0.7463, 0.4136, 0.6773, 0.5149, 0.6120],\n",
      "        [0.2966, 0.0934, 0.4180, 0.2091, 0.4412, 0.5744, 0.7833, 0.3161, 0.0822,\n",
      "         0.2045, 0.9232, 0.0873, 0.8688, 0.2684, 0.5810, 0.6176, 0.3799, 0.3600,\n",
      "         0.5485, 0.8891, 0.6618, 0.7756, 0.2537, 0.5864, 0.3227, 0.4851, 0.3880]])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.2965766191482544\n",
      "log likelihood: -1.2154496908187866\n",
      "negative log likelihood: 1.2154496908187866\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 0.6165539026260376\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "  # i-th bigram:\n",
    "        x = xs[i][j].item() # input character index\n",
    "        y = ys[i].item() # label character index\n",
    "        print('--------')\n",
    "        print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "        print('input to the neural net:', x)\n",
    "        print('output probabilities from the neural net:', probs[i])\n",
    "        print('label (actual next character):', y)\n",
    "        p = probs[i,j,y]\n",
    "        print('probability assigned by the net to the the correct character:', p.item())\n",
    "        logp = torch.log(p)\n",
    "        print('log likelihood:', logp.item())\n",
    "        nll = -logp\n",
    "        print('negative log likelihood:', nll.item())\n",
    "        nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01991f7-781e-43f5-a3e1-f76e23c59e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL TOGETHER NOW!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "add75b1d-3d8f-4e6f-b881-b03a28fa4aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "for word in words:\n",
    "    word = '.' + word + '.'\n",
    "    tuples = [([word[i],word[i+1]], word[i+2]) for i in range(len(word)-2)]\n",
    "    for [x1,x2],y in tuples:\n",
    "        ix1 = stoi[x1]\n",
    "        ix2 = stoi[x2]\n",
    "        iy = stoi[y]\n",
    "        xs.append((ix1,ix2))\n",
    "        ys.append(iy)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "ff630cf9-69b6-4f10-84c2-0e6ef8938875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "e7f40c01-fbe5-4bbc-834b-177d65a9b6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580dc3e0-58a5-43bb-b8fe-8af689709a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 2])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "81974c4d-8d0a-487c-909e-10cd1f0a8dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6039958000183105\n",
      "2.59623384475708\n",
      "2.593493700027466\n",
      "2.5934407711029053\n",
      "2.593430995941162\n",
      "2.5934269428253174\n",
      "2.593424081802368\n",
      "2.593421697616577\n",
      "2.5934197902679443\n",
      "2.5934181213378906\n",
      "2.593416690826416\n",
      "2.5934150218963623\n",
      "2.5934133529663086\n",
      "2.593412399291992\n",
      "2.5934109687805176\n",
      "2.593409538269043\n",
      "2.5934083461761475\n",
      "2.593407154083252\n",
      "2.5934059619903564\n",
      "2.593404769897461\n",
      "2.5934035778045654\n",
      "2.59340238571167\n",
      "2.5934009552001953\n",
      "2.5933997631073\n",
      "2.5933985710144043\n",
      "2.593397378921509\n",
      "2.5933966636657715\n",
      "2.593395471572876\n",
      "2.5933942794799805\n",
      "2.593392848968506\n",
      "2.5933918952941895\n",
      "2.593390941619873\n",
      "2.5933897495269775\n",
      "2.593388557434082\n",
      "2.5933878421783447\n",
      "2.593386650085449\n",
      "2.593385696411133\n",
      "2.5933847427368164\n",
      "2.593383550643921\n",
      "2.5933825969696045\n",
      "2.593381404876709\n",
      "2.5933806896209717\n",
      "2.593379259109497\n",
      "2.5933783054351807\n",
      "2.5933775901794434\n",
      "2.593376636505127\n",
      "2.5933754444122314\n",
      "2.593374490737915\n",
      "2.5933735370635986\n",
      "2.593372344970703\n",
      "2.593371629714966\n",
      "2.5933709144592285\n",
      "2.593369483947754\n",
      "2.5933687686920166\n",
      "2.593367576599121\n",
      "2.593367099761963\n",
      "2.5933659076690674\n",
      "2.593364953994751\n",
      "2.5933642387390137\n",
      "2.5933632850646973\n",
      "2.593362331390381\n",
      "2.5933611392974854\n",
      "2.593360662460327\n",
      "2.5933597087860107\n",
      "2.5933587551116943\n",
      "2.593358039855957\n",
      "2.5933570861816406\n",
      "2.5933563709259033\n",
      "2.593355417251587\n",
      "2.5933547019958496\n",
      "2.593353748321533\n",
      "2.593353033065796\n",
      "2.5933520793914795\n",
      "2.593351364135742\n",
      "2.5933501720428467\n",
      "2.5933494567871094\n",
      "2.593348741531372\n",
      "2.5933477878570557\n",
      "2.5933473110198975\n",
      "2.59334659576416\n",
      "2.5933454036712646\n",
      "2.5933446884155273\n",
      "2.59334397315979\n",
      "2.5933432579040527\n",
      "2.5933423042297363\n",
      "2.593341588973999\n",
      "2.5933408737182617\n",
      "2.5933403968811035\n",
      "2.593339443206787\n",
      "2.5933384895324707\n",
      "2.5933380126953125\n",
      "2.593337297439575\n",
      "2.593336343765259\n",
      "2.5933356285095215\n",
      "2.593334913253784\n",
      "2.593334436416626\n",
      "2.5933337211608887\n",
      "2.5933327674865723\n",
      "2.593331813812256\n",
      "2.5933313369750977\n",
      "2.5933308601379395\n",
      "2.593330144882202\n",
      "2.593329429626465\n",
      "2.5933287143707275\n",
      "2.5933282375335693\n",
      "2.593327045440674\n",
      "2.5933268070220947\n",
      "2.5933258533477783\n",
      "2.59332537651062\n",
      "2.593324899673462\n",
      "2.5933239459991455\n",
      "2.5933234691619873\n",
      "2.593322515487671\n",
      "2.5933220386505127\n",
      "2.5933210849761963\n",
      "2.593320608139038\n",
      "2.593319892883301\n",
      "2.5933194160461426\n",
      "2.5933189392089844\n",
      "2.593318223953247\n",
      "2.593317747116089\n",
      "2.5933167934417725\n",
      "2.5933163166046143\n",
      "2.593315601348877\n",
      "2.593315362930298\n",
      "2.5933144092559814\n",
      "2.5933139324188232\n",
      "2.593312978744507\n",
      "2.593312978744507\n",
      "2.5933120250701904\n",
      "2.593311309814453\n",
      "2.593311071395874\n",
      "2.5933103561401367\n",
      "2.5933096408843994\n",
      "2.593309164047241\n",
      "2.593308687210083\n",
      "2.5933079719543457\n",
      "2.5933074951171875\n",
      "2.59330677986145\n",
      "2.593306064605713\n",
      "2.593305826187134\n",
      "2.5933051109313965\n",
      "2.593304395675659\n",
      "2.59330415725708\n",
      "2.593303680419922\n",
      "2.5933029651641846\n",
      "2.5933024883270264\n",
      "2.593302011489868\n",
      "2.593301296234131\n",
      "2.5933008193969727\n",
      "2.5933003425598145\n",
      "2.593299627304077\n",
      "2.59329891204834\n",
      "2.59329891204834\n",
      "2.5932984352111816\n",
      "2.5932977199554443\n",
      "2.5932974815368652\n",
      "2.593296527862549\n",
      "2.5932958126068115\n",
      "2.5932955741882324\n",
      "2.593295097351074\n",
      "2.593294858932495\n",
      "2.5932939052581787\n",
      "2.5932936668395996\n",
      "2.5932929515838623\n",
      "2.593292713165283\n",
      "2.593291997909546\n",
      "2.5932915210723877\n",
      "2.5932912826538086\n",
      "2.5932908058166504\n",
      "2.593290090560913\n",
      "2.593289613723755\n",
      "2.5932891368865967\n",
      "2.5932886600494385\n",
      "2.5932884216308594\n",
      "2.593287706375122\n",
      "2.593287467956543\n",
      "2.5932867527008057\n",
      "2.5932865142822266\n",
      "2.5932860374450684\n",
      "2.5932857990264893\n",
      "2.593285083770752\n",
      "2.5932846069335938\n",
      "2.5932841300964355\n",
      "2.5932836532592773\n",
      "2.5932834148406982\n",
      "2.593282699584961\n",
      "2.593282461166382\n",
      "2.5932817459106445\n",
      "2.5932817459106445\n",
      "2.5932810306549072\n",
      "2.59328031539917\n",
      "2.59328031539917\n",
      "2.5932796001434326\n",
      "2.5932793617248535\n",
      "2.5932788848876953\n",
      "2.5932788848876953\n",
      "2.593277931213379\n",
      "2.5932774543762207\n",
      "2.5932774543762207\n",
      "2.5932767391204834\n",
      "2.5932765007019043\n",
      "2.593275785446167\n",
      "2.593275547027588\n",
      "2.593275308609009\n",
      "2.5932750701904297\n",
      "2.5932745933532715\n",
      "2.5932741165161133\n",
      "2.593273639678955\n",
      "2.593273401260376\n",
      "2.5932729244232178\n",
      "2.5932724475860596\n",
      "2.5932719707489014\n",
      "2.593271493911743\n",
      "2.593271255493164\n",
      "2.593270778656006\n",
      "2.5932703018188477\n",
      "2.5932700634002686\n",
      "2.5932695865631104\n",
      "2.5932693481445312\n",
      "2.593268871307373\n",
      "2.593268394470215\n",
      "2.5932681560516357\n",
      "2.5932676792144775\n",
      "2.5932672023773193\n",
      "2.5932672023773193\n",
      "2.593266725540161\n",
      "2.593266248703003\n",
      "2.593266248703003\n",
      "2.5932655334472656\n",
      "2.5932652950286865\n",
      "2.5932650566101074\n",
      "2.593264579772949\n",
      "2.59326434135437\n",
      "2.593263864517212\n",
      "2.5932633876800537\n",
      "2.5932631492614746\n",
      "2.5932629108428955\n",
      "2.5932624340057373\n",
      "2.593262195587158\n",
      "2.593261957168579\n",
      "2.593261480331421\n",
      "2.593261241912842\n",
      "2.5932605266571045\n",
      "2.5932605266571045\n",
      "2.5932602882385254\n",
      "2.593259572982788\n",
      "2.593259572982788\n",
      "2.593259334564209\n",
      "2.5932586193084717\n",
      "2.5932586193084717\n",
      "2.5932581424713135\n",
      "2.5932579040527344\n",
      "2.5932576656341553\n",
      "2.593257188796997\n",
      "2.593256711959839\n",
      "2.5932564735412598\n",
      "2.5932562351226807\n",
      "2.5932559967041016\n",
      "2.5932557582855225\n",
      "2.5932552814483643\n",
      "2.593254804611206\n",
      "2.593255043029785\n",
      "2.593254327774048\n",
      "2.5932540893554688\n",
      "2.5932536125183105\n",
      "2.5932533740997314\n",
      "2.5932528972625732\n",
      "2.5932528972625732\n",
      "2.593252420425415\n",
      "2.593252182006836\n",
      "2.593251943588257\n",
      "2.5932514667510986\n",
      "2.5932512283325195\n",
      "2.5932507514953613\n",
      "2.5932507514953613\n",
      "2.593250036239624\n",
      "2.593249797821045\n",
      "2.593249797821045\n",
      "2.593249559402466\n",
      "2.5932490825653076\n",
      "2.5932488441467285\n",
      "2.593248128890991\n",
      "2.5932486057281494\n",
      "2.593247890472412\n",
      "2.593247652053833\n",
      "2.593247413635254\n",
      "2.593247175216675\n",
      "2.5932469367980957\n",
      "2.5932464599609375\n",
      "2.5932464599609375\n",
      "2.5932462215423584\n",
      "2.5932457447052\n",
      "2.593245506286621\n",
      "2.593245267868042\n",
      "2.593245029449463\n",
      "2.593244791030884\n",
      "2.5932443141937256\n",
      "2.5932445526123047\n",
      "2.5932438373565674\n"
     ]
    }
   ],
   "source": [
    "num = len(xs)\n",
    "for k in range(300):\n",
    "    xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "    #xenc = F.one_hot(xs, num_classes=27*27).float() # input to the network: one-hot encoding\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(-1, keepdims=True) # probabilities for next character\n",
    "    valid_indices = (ys >= 0) & (ys < 27)\n",
    "    loss = -probs[torch.arange(num)[valid_indices], :, ys[valid_indices]].log().mean() + 0.01*(W**2).mean()\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "f640b864-dfc5-4fba-a10f-4cfd01ac6c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mn\n",
      "univrya\n",
      "ri\n",
      "rd\n",
      "iuram\n",
      "anegachainalmcye\n",
      "au\n",
      "ey\n",
      "iklen\n",
      "ni\n",
      "a\n",
      "eikavr\n",
      "hhnte\n",
      "usi\n",
      "atrynalmlynalaraiowo\n",
      "ri\n",
      "wele\n",
      "endmielayayqzahihynaylortrakriyniamchraicrcaleth\n",
      "o\n",
      "s\n",
      "ydm\n",
      "aon\n",
      "i\n",
      "e\n",
      "oi\n",
      "aahslaisoa\n",
      "ieeayaalalasaynl\n",
      "uaker\n",
      "on\n",
      "h\n",
      "alelqokorldeimaerleljdsahzllelolnauleyan\n",
      "eha\n",
      "ariosruvnwcyn\n",
      "usia\n",
      "ih\n",
      "ha\n",
      "as\n",
      "uthailoeoxyn\n",
      "ihvanain\n",
      "tihniln\n",
      "arno\n",
      "madiasnn\n",
      "rih\n",
      "e\n",
      "ac\n",
      "ubiun\n",
      "av\n",
      "anolama\n",
      "ope\n",
      "a\n",
      "yndmu\n",
      "yely\n",
      "i\n",
      "ianeteo\n",
      "uahhaio\n",
      "h\n",
      "maletfarahaajaaclaien\n",
      "are\n",
      "al\n",
      "o\n",
      "avemanin\n",
      "yidlaeto\n",
      "el\n",
      "a\n",
      "ureen\n",
      "ambyaliesoukpnky\n",
      "iladamoeastz\n",
      "ae\n",
      "hlan\n",
      "e\n",
      "e\n",
      "atn\n",
      "aotsmar\n",
      "r\n",
      "alya\n",
      "axzee\n",
      "otyanichsa\n",
      "ontinteri\n",
      "a\n",
      "zitrian\n",
      "ionadaoetlleioraxde\n",
      "daiaan\n",
      "ylisel\n",
      "his\n",
      "au\n",
      "veviaamreynsaiy\n",
      "aiaayleblaaxtamrbrwdeln\n",
      "manayn\n",
      "aen\n",
      "ai\n",
      "rhfars\n",
      "a\n",
      "a\n",
      "oo\n",
      "ainy\n",
      "eha\n",
      "yahadnn\n",
      "oeiyneyarllninir\n",
      "n\n",
      "ariayaisenyxfxghetma\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for i in range(100):\n",
    "    out = []\n",
    "    ix = [stoi['.'], stoi['.']]\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        p = counts / counts.sum(-1, keepdims=True) # probabilities for next character\n",
    "        ix = torch.multinomial(p[-1], num_samples=1, replacement=True, generator=g)[0].item()\n",
    "        out.append(itos[ix])\n",
    "        input_seq = torch.cat((input_seq[1:], torch.tensor([ix], dtype=torch.long)))\n",
    "        if ix == 0:\n",
    "            break\n",
    "        ix = [ix, stoi['.']] # shift the sequence by one character\n",
    "    print(''.join(out[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0454802a-cfe1-4d78-b070-bec086bbbc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
